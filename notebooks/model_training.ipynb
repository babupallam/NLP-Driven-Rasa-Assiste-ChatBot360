{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Alright, let's proceed with the **Seq2Seq Model Implementation** for building the foundational chatbot using an Encoder-Decoder architecture. Below, I've included a detailed impl\n",
    "### **1.2 Seq2Seq Model Implementation** in `model_training.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "#### **Section 1: Import Libraries**\n",
    "\n",
    "We start by importing the necessary libraries for data handling, deep learning, and other utilities.\n",
    "\n"
   ],
   "id": "2e75f2da2ce2abd0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Download the Punkt tokenizer (used for splitting sentences into words)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Observations:\n",
    "# - PyTorch is used for creating and training the neural network.\n",
    "# - NLTK is used for preprocessing and tokenizing text.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **Section 2: Load and Prepare Data**\n",
    "\n",
    "Load the cleaned dataset that was saved in the previous preprocessing step (`customer_support_dataset_processed.csv`) and prepare it for training.\n",
    "\n"
   ],
   "id": "6dcc650cdf0e0c18"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 2: Load the Processed Data\n",
    "file_path = \"../data/processed/customer_support_dataset_processed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Split data into input and output pairs\n",
    "queries = df['customer_query_cleaned']\n",
    "responses = df['support_response_cleaned']\n",
    "\n",
    "# Split dataset into training and validation sets (90% train, 10% validation)\n",
    "train_queries, val_queries, train_responses, val_responses = train_test_split(\n",
    "    queries, responses, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Observations:\n",
    "# - The cleaned dataset is loaded, and customer queries are paired with their responses.\n",
    "# - The data is split into training and validation sets for training and evaluating the model's performance.\n"
   ],
   "id": "75d03ab8849a9fcd",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **Section 3: Text Tokenization and Vocabulary Building**\n",
    "\n",
    "Tokenize the sentences and build vocabulary dictionaries for mapping words to integer tokens.\n",
    "\n"
   ],
   "id": "c037ca552bc214fb"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Tokenize the text and create vocabulary\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Step 3.1: Tokenization\n",
    "train_queries_tokens = [word_tokenize(query) for query in train_queries]\n",
    "train_responses_tokens = [word_tokenize(response) for response in train_responses]\n",
    "\n",
    "# Step 3.2: Building Vocabulary\n",
    "def build_vocab(token_lists):\n",
    "    vocab = Counter()\n",
    "    for tokens in token_lists:\n",
    "        vocab.update(tokens)\n",
    "    return vocab\n",
    "\n",
    "vocab_queries = build_vocab(train_queries_tokens)\n",
    "vocab_responses = build_vocab(train_responses_tokens)\n",
    "\n",
    "# Add special tokens to vocabulary\n",
    "special_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "for token in special_tokens:\n",
    "    vocab_queries[token] = float('inf')\n",
    "    vocab_responses[token] = float('inf')\n",
    "\n",
    "# Create word to index and index to word dictionaries\n",
    "word2idx = {word: idx for idx, (word, _) in enumerate(vocab_queries.items())}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# Observations:\n",
    "# - Text is tokenized using the NLTK tokenizer.\n",
    "# - Vocabulary is built using word frequency counting.\n",
    "# - Special tokens like '<PAD>' (padding), '<SOS>' (start of sentence), '<EOS>' (end of sentence), and '<UNK>' (unknown word) are added.\n"
   ],
   "id": "3f3004d6c087e04c",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **Section 4: Create Custom Dataset and DataLoader**\n",
    "\n",
    "Prepare a PyTorch dataset and DataLoader to handle batching during training.\n",
    "\n"
   ],
   "id": "f662c4e8081601c5"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 4: Custom Dataset and DataLoader\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, queries, responses, word2idx, max_len=20):\n",
    "        self.queries = queries\n",
    "        self.responses = responses\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert text to token ids and pad/truncate to max_len\n",
    "        query = self._text_to_sequence(self.queries[idx])\n",
    "        response = self._text_to_sequence(self.responses[idx])\n",
    "        return torch.tensor(query, dtype=torch.long), torch.tensor(response, dtype=torch.long)\n",
    "\n",
    "    def _text_to_sequence(self, text):\n",
    "        tokens = word_tokenize(text)\n",
    "        sequence = [self.word2idx.get(token, self.word2idx['<UNK>']) for token in tokens]\n",
    "        sequence = [self.word2idx['<SOS>']] + sequence + [self.word2idx['<EOS>']]\n",
    "        # Pad/truncate to max_len\n",
    "        sequence = sequence[:self.max_len] + [self.word2idx['<PAD>']] * (self.max_len - len(sequence))\n",
    "        return sequence\n",
    "\n",
    "# Create DataLoader instances for training and validation datasets\n",
    "train_dataset = ChatDataset(train_queries, train_responses, word2idx)\n",
    "val_dataset = ChatDataset(val_queries, val_responses, word2idx)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Observations:\n",
    "# - A custom PyTorch dataset (`ChatDataset`) is created to handle tokenization and padding.\n",
    "# - Padding ensures that all sequences have the same length, facilitating easy batch processing.\n",
    "# - DataLoader is used to create batches for training and validation, which helps to efficiently manage large datasets.\n"
   ],
   "id": "3091c51ad71de2dc",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### **Section 5: Encoder-Decoder Model Design**\n",
    "\n",
    "Define the Encoder and Decoder classes for the Seq2Seq architecture.\n",
    "\n",
    "##### **5.1 Encoder Class**\n",
    "\n"
   ],
   "id": "eab1e86d7c52ad56"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 5.1: Encoder Definition\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "# Observations:\n",
    "# - The Encoder takes the input sentence and produces hidden and cell states.\n",
    "# - The embedding layer converts word indices to embedding vectors.\n",
    "# - The LSTM (or GRU) captures sequential dependencies.\n"
   ],
   "id": "939c3afd72e34fc9",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### **5.2 Decoder Class**\n",
    "\n"
   ],
   "id": "67674d72ef93f897"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 5.2: Decoder Definition\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_dim, hidden_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = x.unsqueeze(1)  # Add time dimension\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "# Observations:\n",
    "# - The Decoder takes the previous word, hidden state, and cell state to produce the next word.\n",
    "# - The fully connected layer is used to predict the next word in the sequence.\n"
   ],
   "id": "bc5add52a0f8320c",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "##### **5.3 Seq2Seq Model Class**\n",
    "\n"
   ],
   "id": "bc3c0e0c9369114b"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 5.3: Seq2Seq Model Class\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size = source.shape[0]\n",
    "        target_len = target.shape[1]\n",
    "        output_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, target_len, output_size).to(self.device)\n",
    "\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # Take first word input as <SOS>\n",
    "        input = target[:, 0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t, :] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = target[:, t] if np.random.random() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Observations:\n",
    "# - The Seq2Seq class ties the Encoder and Decoder together.\n",
    "# - Teacher forcing is used during training to decide whether to use the model's prediction or the true target.\n"
   ],
   "id": "5bdab3eec446ee45",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### **Section 6: Training the Model**\n",
    "\n",
    "Define the training process, including the loss function and optimizer.\n",
    "\n"
   ],
   "id": "7d9e2c5fd238a550"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 6: Training the Model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = len(word2idx)\n",
    "output_size = len(word2idx)\n",
    "embedding_dim = 256\n",
    "hidden_size = 512\n",
    "\n",
    "# Instantiate encoder, decoder, and Seq2Seq model\n",
    "encoder = Encoder(input_size, embedding_dim, hidden_size).to(device)\n",
    "decoder = Decoder(output_size, embedding_dim, hidden_size).to(device)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "\n",
    "# Observations:\n",
    "# - CrossEntropyLoss is used to compare predicted and actual words, ignoring padding tokens.\n",
    "# - An optimizer like Adam is chosen to improve the training efficiency.\n"
   ],
   "id": "9b2a1ff4e2188428",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6251c82307d432ec"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "id": "4ad820925a16868f",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "14186366a39e7577"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "id": "55f70ee4ed323068",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f5f40c097e37b3f3"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "id": "f8ca0b8d579ba7e5",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9bf0fa726c9116b4"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "id": "5cc11740f49df840",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
