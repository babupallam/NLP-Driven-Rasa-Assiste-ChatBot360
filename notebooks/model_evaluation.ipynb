{
 "cells": [
  {
   "metadata": {
    "id": "2e75f2da2ce2abd0"
   },
   "cell_type": "markdown",
   "source": [
    "#### **Section 1: Import Libraries**\n",
    "\n",
    "We import all the necessary libraries. Notice that we no longer need to import `gensim`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "initial_id",
    "outputId": "d3a060b8-619a-40c6-de26-4c390c6996b0",
    "ExecuteTime": {
     "end_time": "2024-11-30T18:09:32.815429Z",
     "start_time": "2024-11-30T18:09:28.507607Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Import word_tokenize explicitly from NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Observations:\n",
    "# - Added an import for `word_tokenize` explicitly after downloading the NLTK 'punkt' resource.\n",
    "# - The word_tokenize function is now available globally in the script, and the error will no longer occur.\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Girija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "id": "6dcc650cdf0e0c18"
   },
   "cell_type": "markdown",
   "source": [
    "#### **Section 2: Load and Prepare Data**\n",
    "\n",
    "The data loading process remains the same.\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "75d03ab8849a9fcd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "outputId": "9a7e72ef-d851-4100-af36-8b23f0042b16",
    "ExecuteTime": {
     "end_time": "2024-11-30T18:34:20.352194Z",
     "start_time": "2024-11-30T18:34:18.720463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Load the Processed Data\n",
    "#file_path = \"../data/processed/customer_support_dataset_processed.csv\"  # for complete set\n",
    "file_path = \"../data/processed/customer_support_test_dataset_processed_10%.csv\" # for 10% for the complete set - for simple training\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the dataset contains essential columns\n",
    "if 'customer_query_cleaned' not in df.columns or 'support_response_cleaned' not in df.columns:\n",
    "    raise ValueError(\"Dataset missing required columns: 'customer_query_cleaned' and 'support_response_cleaned'\")\n",
    "\n",
    "# Split data into input and output pairs\n",
    "queries = df['customer_query_cleaned']\n",
    "responses = df['support_response_cleaned']\n",
    "\n",
    "# Split dataset into training and validation sets (90% train, 10% validation)\n",
    "train_queries, val_queries, train_responses, val_responses = train_test_split(\n",
    "    queries, responses, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Observations:\n",
    "# - Loaded and validated the cleaned dataset.\n",
    "# - Split the data into training and validation sets, which is critical for model evaluation and avoiding overfitting.\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "id": "c037ca552bc214fb"
   },
   "cell_type": "markdown",
   "source": [
    "#### **Section 3: Load Pre-trained GloVe Embeddings Without Gensim**\n",
    "\n",
    "Instead of using Gensim, you will manually download the GloVe embeddings, read them, and then use them to create the embedding matrix.\n",
    "\n",
    "##### **3.1 Download GloVe Embeddings Manually**\n",
    "\n",
    "- You can download GloVe embeddings manually from the [GloVe Website](https://nlp.stanford.edu/projects/glove/). Choose, for example, the **glove.6B.zip** file and extract it.\n",
    "- It contains multiple files like `glove.6B.50d.txt`, `glove.6B.100d.txt`, etc. We'll use `glove.6B.100d.txt` for 100-dimensional word embeddings.\n",
    "\n",
    "##### **3.2 Load GloVe Embeddings in Python**\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "3f3004d6c087e04c",
    "ExecuteTime": {
     "end_time": "2024-11-30T18:09:59.182879Z",
     "start_time": "2024-11-30T18:09:33.858988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3: Load Pre-trained GloVe Embeddings (Without Gensim)\n",
    "embedding_dim = 100\n",
    "glove_path = \"../glove.6B.100d.txt\"  # Path to the downloaded GloVe file\n",
    "\n",
    "# Initialize word2idx and embedding matrix lists\n",
    "word2idx = {}\n",
    "embedding_matrix = []\n",
    "\n",
    "# Open the GloVe file and read the embeddings\n",
    "print(\"Loading pre-trained GloVe embeddings (may take a few minutes)...\")\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        word2idx[word] = idx\n",
    "        embedding_matrix.append(vector)\n",
    "\n",
    "# Add special tokens with random embeddings\n",
    "special_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "for token in special_tokens:\n",
    "    word2idx[token] = len(embedding_matrix)\n",
    "    embedding_matrix.append(np.random.normal(size=(embedding_dim,)))\n",
    "\n",
    "# Convert embedding matrix to a tensor\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Observations:\n",
    "# - Loaded GloVe embeddings manually using Python without Gensim.\n",
    "# - Created an embedding matrix and added random embeddings for special tokens.\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained GloVe embeddings (may take a few minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Girija\\AppData\\Local\\Temp\\ipykernel_22120\\1172931956.py:26: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "id": "f662c4e8081601c5"
   },
   "cell_type": "markdown",
   "source": [
    "#### **Section 4: Custom Dataset and DataLoader**\n",
    "\n",
    "This section remains the same. The custom dataset is responsible for tokenizing the input and padding it to a fixed length.\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "3091c51ad71de2dc",
    "ExecuteTime": {
     "end_time": "2024-11-30T18:09:59.724243Z",
     "start_time": "2024-11-30T18:09:59.619848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 4: Custom Dataset and DataLoader\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, queries, responses, word2idx, max_len=20):\n",
    "        # Reset the index of queries and responses to ensure valid indexing\n",
    "        self.queries = queries.reset_index(drop=True).fillna(\"\")  # Replace NaN with empty string\n",
    "        self.responses = responses.reset_index(drop=True).fillna(\"\")  # Replace NaN with empty string\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert text to token ids and pad/truncate to max_len\n",
    "        try:\n",
    "            query = self._text_to_sequence(self.queries[idx])\n",
    "            response = self._text_to_sequence(self.responses[idx])\n",
    "        except KeyError:\n",
    "            print(f\"KeyError: Index {idx} out of bounds for dataset length {len(self.queries)}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error at index {idx}: {e}\")\n",
    "            raise\n",
    "        return torch.tensor(query, dtype=torch.long), torch.tensor(response, dtype=torch.long)\n",
    "\n",
    "    def _text_to_sequence(self, text):\n",
    "        # Handle non-string inputs\n",
    "        if not isinstance(text, str):\n",
    "            print(f\"Invalid input detected: {text} (type: {type(text)}). Converting to empty string.\")\n",
    "            text = \"\"\n",
    "\n",
    "        tokens = word_tokenize(text)  # Tokenize the text\n",
    "        sequence = [self.word2idx.get(token, self.word2idx['<UNK>']) for token in tokens]\n",
    "        sequence = [self.word2idx['<SOS>']] + sequence + [self.word2idx['<EOS>']]\n",
    "        sequence = sequence[:self.max_len] + [self.word2idx['<PAD>']] * (self.max_len - len(sequence))\n",
    "        return sequence\n",
    "\n",
    "# DataLoader instances for training and validation\n",
    "train_dataset = ChatDataset(train_queries, train_responses, word2idx)\n",
    "val_dataset = ChatDataset(val_queries, val_responses, word2idx)\n",
    "\n",
    "# DataLoader setup with reduced batch size to lower memory consumption\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0, pin_memory=True)\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "id": "eab1e86d7c52ad56"
   },
   "cell_type": "markdown",
   "source": [
    "#### **Section 5: Encoder-Decoder Model Design with Attention**\n",
    "\n",
    "The encoder and decoder design with attention remains largely unchanged, except that we use the manually loaded GloVe embeddings.\n",
    "\n",
    "##### **5.1 Encoder Definition**\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "939c3afd72e34fc9",
    "ExecuteTime": {
     "end_time": "2024-11-30T18:10:00.113963Z",
     "start_time": "2024-11-30T18:10:00.099960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encoder Definition\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_matrix, hidden_size, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.rnn = nn.LSTM(embedding_matrix.size(1), hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        embedded = self.embedding(x)  # embedded: [batch_size, seq_len, embedding_dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# Observations:\n",
    "# - The encoder uses pre-trained GloVe embeddings loaded manually.\n",
    "# - The embeddings are not frozen (`freeze=False`), meaning they will be fine-tuned during training.\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "id": "67674d72ef93f897"
   },
   "cell_type": "markdown",
   "source": [
    "##### **5.2 Decoder with Attention Definition**\n",
    "\n",
    "The decoder is modified to include the attention layer for better context representation.\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "bc5add52a0f8320c",
    "ExecuteTime": {
     "end_time": "2024-11-30T18:10:00.516147Z",
     "start_time": "2024-11-30T18:10:00.488983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Attention Mechanism (Make sure it's defined before the decoder)\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Linear layers to compute alignment scores and convert to attention weights\n",
    "        self.attention = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [batch_size, hidden_size]\n",
    "        # encoder_outputs: [batch_size, seq_len, hidden_size]\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        seq_len = encoder_outputs.shape[1]\n",
    "\n",
    "        # Repeat hidden state seq_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "\n",
    "        # Concatenate encoder outputs with the repeated hidden state\n",
    "        energy = torch.tanh(self.attention(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # Calculate attention scores\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "\n",
    "        # Apply softmax to calculate attention weights\n",
    "        return torch.softmax(attention, dim=1)\n",
    "\n",
    "# Decoder with Attention Definition (After Attention class is defined)\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, output_size, embedding_matrix, hidden_size, num_layers=1):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size + embedding_matrix.size(1), hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell, encoder_outputs):\n",
    "        # x: [batch_size], hidden, cell: [num_layers, batch_size, hidden_size], encoder_outputs: [batch_size, seq_len, hidden_size]\n",
    "        x = x.unsqueeze(1)  # Add time dimension: [batch_size, 1]\n",
    "        embedded = self.embedding(x)  # embedded: [batch_size, 1, embedding_dim]\n",
    "\n",
    "        # Calculate attention weights and apply to encoder outputs to get context vector\n",
    "        attention_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "        attention_weights = attention_weights.unsqueeze(1)  # [batch_size, 1, seq_len]\n",
    "        context = torch.bmm(attention_weights, encoder_outputs)  # [batch_size, 1, hidden_size]\n",
    "\n",
    "        # Concatenate the context vector with the embedded input word\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)  # [batch_size, 1, hidden_size + embedding_dim]\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "\n",
    "        # Use the output of RNN and context vector for prediction\n",
    "        prediction = self.fc(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))  # [batch_size, output_size]\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "# Observations:\n",
    "# - The `Attention` class must be defined before it is used by `DecoderWithAttention`.\n",
    "# - This ensures there is no `NameError` when defining the decoder.\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "id": "bc3c0e0c9369114b"
   },
   "cell_type": "markdown",
   "source": [
    "#### **Section 6: Seq2Seq Model Class with Attention Decoder**\n",
    "\n",
    "The Seq2Seq class integrates the **Encoder** and **DecoderWithAttention** to generate responses.\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "5bdab3eec446ee45",
    "ExecuteTime": {
     "end_time": "2024-11-30T18:10:00.906428Z",
     "start_time": "2024-11-30T18:10:00.879650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Seq2Seq Model Class with Attention Decoder\n",
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2SeqWithAttention, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size = source.shape[0]\n",
    "        target_len = target.shape[1]\n",
    "        output_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, target_len, output_size).to(self.device)\n",
    "\n",
    "        # Pass input through the encoder\n",
    "        encoder_outputs, hidden, cell = self.encoder(source)\n",
    "\n",
    "        # First input to the decoder is the <SOS> token\n",
    "        input = target[:, 0]  # <SOS> token for each batch\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Pass the input, hidden state, and encoder outputs to the decoder\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            # Determine the next input using teacher forcing\n",
    "            top1 = output.argmax(1)\n",
    "            input = target[:, t] if np.random.random() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Observations:\n",
    "# - The Seq2Seq model class integrates the encoder and decoder and passes encoder outputs to the decoder for attention.\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "id": "9bf0fa726c9116b4"
   },
   "cell_type": "markdown",
   "source": [
    "#### Step 9: Evaluate and Test the Trained Model\n",
    "\n",
    "##### Steps:\n",
    "- Load the Trained Model: Load the best model checkpoint saved during training.\n",
    "- Put the Model in Evaluation Mode: Set the model to evaluation mode using model.eval().\n",
    "- Define Metrics for Evaluation:\n",
    "    - Calculate BLEU score to measure the quality of generated responses.\n",
    "    - Generate Responses: For a given input, generate responses and compare them with the ground truth.\n",
    "    - Evaluate with Test Data: Loop over a test dataset, generate responses, and calculate evaluation metrics."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T18:37:22.869126Z",
     "start_time": "2024-11-30T18:37:21.202401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "test_data_path = \"../data/processed/customer_support_test_dataset_processed_10%.csv\"\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(test_data.head())\n",
    "\n",
    "# Display columns in the dataset\n",
    "print(test_data.columns)\n",
    "\n",
    "# Check for missing values\n",
    "print(test_data.isnull().sum())\n",
    "\n",
    "# Check data types of each column\n",
    "print(test_data.dtypes)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              customer_query_cleaned  \\\n",
      "0  swhelp dont worry i forgot there was a rmt strike   \n",
      "1  aldiuk please tell me kevin will be back on st...   \n",
      "2                      americanair  httpstcooevjrhfh   \n",
      "3  comcastcares morning  back to deal w our hddta...   \n",
      "4  americanair quite possibly the worst serviceit...   \n",
      "\n",
      "                            support_response_cleaned  \n",
      "0   no probs we got yo back fam if anything happe...  \n",
      "1   thanks catherine ive passed this onto our dut...  \n",
      "2   yes we are awaiting an update as to when the ...  \n",
      "3   sorry to know youre facing issues with cash b...  \n",
      "4   ive just checked and we arent listed on there...  \n",
      "Index(['customer_query_cleaned', 'support_response_cleaned'], dtype='object')\n",
      "customer_query_cleaned      72\n",
      "support_response_cleaned     1\n",
      "dtype: int64\n",
      "customer_query_cleaned      object\n",
      "support_response_cleaned    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "import the necessory things first"
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "5cc11740f49df840",
    "ExecuteTime": {
     "end_time": "2024-11-30T18:41:57.840283Z",
     "start_time": "2024-11-30T18:41:52.536593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "\n",
    "# Ensure nltk resources are available for BLEU calculation\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the trained model checkpoint (Ensure compatibility with CPU-only machines)\n",
    "checkpoint_path = \"../models/seq2seq_attention_best_model.pth\"\n",
    "\n",
    "# Hyperparameters (Ensure these are the same as those used during training)\n",
    "input_size = len(word2idx)\n",
    "output_size = len(word2idx)\n",
    "hidden_size = 256  # Ensure that this matches what was used during training\n",
    "\n",
    "# Instantiate encoder, decoder with attention, and Seq2Seq model\n",
    "encoder = Encoder(input_size, embedding_matrix, hidden_size).to(device)\n",
    "decoder = DecoderWithAttention(output_size, embedding_matrix, hidden_size).to(device)\n",
    "model = Seq2SeqWithAttention(encoder, decoder, device).to(device)\n",
    "\n",
    "# Load the trained model weights in non-strict mode\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device), strict=False)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create idx2word dictionary from word2idx for converting token IDs back to words\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Girija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\Girija\\AppData\\Local\\Temp\\ipykernel_22120\\2728193581.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location=device), strict=False)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load and Prepare the Test Dataset\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T18:42:22.905388Z",
     "start_time": "2024-11-30T18:42:21.118811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load test dataset from CSV file\n",
    "test_data_path = \"../data/processed/customer_support_test_dataset_processed_10%.csv\"\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# Handle missing values by dropping rows containing any NaNs in `customer_query_cleaned` or `support_response_cleaned`\n",
    "test_data.dropna(subset=['customer_query_cleaned', 'support_response_cleaned'], inplace=True)\n",
    "\n",
    "# Extract queries and responses (as Pandas Series)\n",
    "test_queries = test_data['customer_query_cleaned']\n",
    "test_responses = test_data['support_response_cleaned']\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_dataset = ChatDataset(test_queries, test_responses, word2idx)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BLEU Score Evaluation for 1000 Test Inputs"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T19:01:24.602241Z",
     "start_time": "2024-11-30T18:42:29.503529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# BLEU Score Evaluation for First 1000 Inputs\n",
    "bleu_scores_1000 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=1000, desc=\"Evaluating First 1000 Inputs\", unit=\"sample\") as pbar:\n",
    "        for i, (source, target) in enumerate(test_loader):\n",
    "            if i >= 1000:\n",
    "                break\n",
    "\n",
    "            source = source.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Get the output from the model\n",
    "            output_tokens = []\n",
    "            encoder_outputs, hidden, cell = model.encoder(source)\n",
    "\n",
    "            # Start the decoding process with <SOS> token\n",
    "            input_token = torch.tensor([word2idx['<SOS>']], dtype=torch.long).to(device)\n",
    "            for _ in range(target.size(1)):\n",
    "                output, hidden, cell = model.decoder(input_token, hidden, cell, encoder_outputs)\n",
    "                top1 = output.argmax(1)\n",
    "                output_tokens.append(top1.item())\n",
    "\n",
    "                # Break if <EOS> token is predicted\n",
    "                if top1.item() == word2idx['<EOS>']:\n",
    "                    break\n",
    "\n",
    "                # The next input token is the current output token\n",
    "                input_token = top1\n",
    "\n",
    "            # Convert predicted token IDs to words\n",
    "            predicted_sentence = [idx2word[token] for token in output_tokens if token != word2idx['<PAD>']]\n",
    "\n",
    "            # Convert target token IDs to words (ground truth)\n",
    "            target_sentence = [idx2word[token.item()] for token in target[0] if token.item() not in [word2idx['<PAD>'], word2idx['<SOS>'], word2idx['<EOS>']]]\n",
    "\n",
    "            # Calculate BLEU score\n",
    "            bleu_score = sentence_bleu([target_sentence], predicted_sentence, weights=(0.5, 0.5))\n",
    "            bleu_scores_1000.append(bleu_score)\n",
    "\n",
    "            pbar.set_postfix({\"BLEU\": bleu_score})\n",
    "            pbar.update(1)\n",
    "\n",
    "# Calculate and print the average BLEU score for the first 1000 test inputs\n",
    "average_bleu_score_1000 = np.mean(bleu_scores_1000)\n",
    "print(f\"Average BLEU Score on First 1000 Test Inputs: {average_bleu_score_1000:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating First 1000 Inputs:   0%|          | 0/1000 [00:00<?, ?sample/s]C:\\Users\\Girija\\Documents\\GitHub\\NLP-Driven-Rasa-Assiste-ChatBot360\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Evaluating First 1000 Inputs: 100%|██████████| 1000/1000 [18:55<00:00,  1.14s/sample, BLEU=0.108]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score on First 1000 Test Inputs: 0.0290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " BLEU Score Evaluation for Full Dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# BLEU Score Evaluation for Full Test Dataset\n",
    "bleu_scores_full = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(test_loader), desc=\"Evaluating Full Test Dataset\", unit=\"sample\") as pbar:\n",
    "        for i, (source, target) in enumerate(test_loader):\n",
    "            source = source.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Get the output from the model\n",
    "            output_tokens = []\n",
    "            encoder_outputs, hidden, cell = model.encoder(source)\n",
    "\n",
    "            # Start the decoding process with <SOS> token\n",
    "            input_token = torch.tensor([word2idx['<SOS>']], dtype=torch.long).to(device)\n",
    "            for _ in range(target.size(1)):\n",
    "                output, hidden, cell = model.decoder(input_token, hidden, cell, encoder_outputs)\n",
    "                top1 = output.argmax(1)\n",
    "                output_tokens.append(top1.item())\n",
    "\n",
    "                # Break if <EOS> token is predicted\n",
    "                if top1.item() == word2idx['<EOS>']:\n",
    "                    break\n",
    "\n",
    "                # The next input token is the current output token\n",
    "                input_token = top1\n",
    "\n",
    "            # Convert predicted token IDs to words\n",
    "            predicted_sentence = [idx2word[token] for token in output_tokens if token != word2idx['<PAD>']]\n",
    "\n",
    "            # Convert target token IDs to words (ground truth)\n",
    "            target_sentence = [idx2word[token.item()] for token in target[0] if token.item() not in [word2idx['<PAD>'], word2idx['<SOS>'], word2idx['<EOS>']]]\n",
    "\n",
    "            # Calculate BLEU score\n",
    "            bleu_score = sentence_bleu([target_sentence], predicted_sentence, weights=(0.5, 0.5))\n",
    "            bleu_scores_full.append(bleu_score)\n",
    "\n",
    "            pbar.set_postfix({\"BLEU\": bleu_score})\n",
    "            pbar.update(1)\n",
    "\n",
    "# Calculate and print the average BLEU score for the full test dataset\n",
    "average_bleu_score_full = np.mean(bleu_scores_full)\n",
    "print(f\"Average BLEU Score on Full Test Dataset: {average_bleu_score_full:.4f}\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Interactive Test with Predefined Inputs: "
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T19:06:27.997690Z",
     "start_time": "2024-11-30T19:06:20.607551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Interactive Test with Predefined Inputs\n",
    "\n",
    "# Predefined test inputs for the chatbot\n",
    "predefined_inputs = [\n",
    "    \"How do I track my order?\",\n",
    "    \"I am facing issues with logging in. Can you help?\",\n",
    "    \"Is my payment secure on your website?\",\n",
    "    \"Where can I find the return policy?\",\n",
    "    \"Can you tell me if my order has been shipped?\"\n",
    "]\n",
    "\n",
    "# Function to clean and tokenize input text for use by the model\n",
    "def prepare_input_text(input_text, word2idx, max_len=20):\n",
    "    tokens = input_text.lower().split()  # Simple tokenization (splitting by space)\n",
    "    token_ids = [word2idx.get(token, word2idx['<UNK>']) for token in tokens]  # Convert tokens to ids\n",
    "    token_ids = token_ids[:max_len]  # Truncate if longer than max_len\n",
    "    token_ids += [word2idx['<PAD>']] * (max_len - len(token_ids))  # Pad if shorter than max_len\n",
    "    return torch.tensor([token_ids], dtype=torch.long).to(device)\n",
    "\n",
    "# Function to generate response from the model\n",
    "def generate_response(model, source_input, word2idx, idx2word):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Encode the source input\n",
    "    encoder_outputs, hidden, cell = model.encoder(source_input)\n",
    "\n",
    "    # Start decoding with <SOS> token\n",
    "    input_token = torch.tensor([word2idx['<SOS>']], dtype=torch.long).to(device)\n",
    "    output_tokens = []\n",
    "\n",
    "    # Decoding loop\n",
    "    max_output_length = 20  # Set a maximum length for the response\n",
    "    for _ in range(max_output_length):\n",
    "        output, hidden, cell = model.decoder(input_token, hidden, cell, encoder_outputs)\n",
    "        top1 = output.argmax(1)\n",
    "        output_tokens.append(top1.item())\n",
    "\n",
    "        # Break if <EOS> token is predicted\n",
    "        if top1.item() == word2idx['<EOS>']:\n",
    "            break\n",
    "\n",
    "        # The next input token is the current output token\n",
    "        input_token = top1\n",
    "\n",
    "    # Convert output tokens to words\n",
    "    response_sentence = [idx2word[token] for token in output_tokens if token != word2idx['<PAD>']]\n",
    "    return ' '.join(response_sentence)\n",
    "\n",
    "# Iterate through predefined customer queries\n",
    "print(\"Interactive Chatbot Responses:\\n\")\n",
    "for customer_query in predefined_inputs:\n",
    "    # Prepare the input text\n",
    "    source_input = prepare_input_text(customer_query, word2idx)\n",
    "\n",
    "    # Generate response using the model\n",
    "    response = generate_response(model, source_input, word2idx, idx2word)\n",
    "\n",
    "    # Display the query and response\n",
    "    print(f\"Customer Query: {customer_query}\")\n",
    "    print(f\"Chatbot Response: {response}\")\n",
    "    print('-' * 50)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive Chatbot Responses:\n",
      "\n",
      "Customer Query: How do I track my order?\n",
      "Chatbot Response: hi there sorry for the trouble please dm us your your email address and we can take a look backstage\n",
      "--------------------------------------------------\n",
      "Customer Query: I am facing issues with logging in. Can you help?\n",
      "Chatbot Response: hi there sorry for the trouble please dm us your your email address and we can take a look backstage\n",
      "--------------------------------------------------\n",
      "Customer Query: Is my payment secure on your website?\n",
      "Chatbot Response: hi there sorry for the trouble please dm us your your email address and we can take a look backstage\n",
      "--------------------------------------------------\n",
      "Customer Query: Where can I find the return policy?\n",
      "Chatbot Response: hi there sorry for the trouble please dm us your your email address and we can take a look backstage\n",
      "--------------------------------------------------\n",
      "Customer Query: Can you tell me if my order has been shipped?\n",
      "Chatbot Response: hi there sorry for the trouble please dm us your your email address and we can take a look backstage\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T19:29:23.789727Z",
     "start_time": "2024-11-30T19:29:13.146896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Optional: Allow for user-inputted queries in real-time\n",
    "while True:\n",
    "    customer_query = input(\"You: \")\n",
    "    if customer_query.lower() in ['exit', 'quit']:\n",
    "        print(\"Exiting the interactive chat...\")\n",
    "        break\n",
    "\n",
    "    # Prepare the input text\n",
    "    source_input = prepare_input_text(customer_query, word2idx)\n",
    "\n",
    "    # Generate response using the model\n",
    "    response = generate_response(model, source_input, word2idx, idx2word)\n",
    "\n",
    "    # Display the chatbot response\n",
    "    print(f\"Chatbot: {response}\")\n",
    "    print('-' * 50)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: hi there sorry for the trouble please dm us your your email address and we can take a look backstage\n",
      "--------------------------------------------------\n",
      "Exiting the interactive chat...\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
