{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T20:06:47.174188Z",
     "start_time": "2024-11-30T20:06:40.908683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% Section 1: Import Libraries and Load Utilities from Files\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "import importlib.util\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Step 1: Set the root directory correctly\n",
    "project_root = os.getcwd()\n",
    "\n",
    "# Debug: Print current working directory to confirm it's set correctly\n",
    "print(f\"Current working directory set to: {project_root}\")\n",
    "\n",
    "# Define the utils path relative to the current directory\n",
    "utils_path = os.path.join(project_root, \"utils\")\n",
    "\n",
    "# Debug: Print the utils path\n",
    "print(f\"Looking for 'utils' folder at: {utils_path}\")\n",
    "\n",
    "# Function to load a module by its file path\n",
    "def load_module(module_name, file_path):\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "# Load encoder.py\n",
    "encoder_path = os.path.join(utils_path, \"encoder.py\")\n",
    "\n",
    "# Debug: Print the encoder path\n",
    "print(f\"Looking for 'encoder.py' at: {encoder_path}\")\n",
    "\n",
    "try:\n",
    "    encoder_module = load_module(\"encoder\", encoder_path)\n",
    "    Encoder = encoder_module.Encoder\n",
    "    print(f\"Encoder loaded successfully from {encoder_path}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. The encoder file path is incorrect. Please verify the file exists.\")\n",
    "    raise\n",
    "\n",
    "# Load decoder.py\n",
    "decoder_path = os.path.join(utils_path, \"decoder.py\")\n",
    "try:\n",
    "    decoder_module = load_module(\"decoder\", decoder_path)\n",
    "    DecoderWithAttention = decoder_module.DecoderWithAttention\n",
    "    print(f\"Decoder loaded successfully from {decoder_path}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. The decoder file path is incorrect.\")\n",
    "    raise\n",
    "\n",
    "# Load seq2seq.py\n",
    "seq2seq_path = os.path.join(utils_path, \"seq2seq.py\")\n",
    "try:\n",
    "    seq2seq_module = load_module(\"seq2seq\", seq2seq_path)\n",
    "    Seq2SeqWithAttention = seq2seq_module.Seq2SeqWithAttention\n",
    "    print(f\"Seq2Seq model loaded successfully from {seq2seq_path}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. The Seq2Seq file path is incorrect.\")\n",
    "    raise\n",
    "\n",
    "# %% Section 2: Load Saved Components\n",
    "\n",
    "# Paths to saved models and data\n",
    "models_path = os.path.join(project_root, \"models\")\n",
    "encoder_weights_path = os.path.join(models_path, \"encoder.pth\")\n",
    "decoder_weights_path = os.path.join(models_path, \"decoder_with_attention.pth\")\n",
    "seq2seq_weights_path = os.path.join(models_path, \"seq2seq_attention_best_model.pth\")\n",
    "embedding_matrix_path = os.path.join(models_path, \"embedding_matrix.pth\")\n",
    "word2idx_path = os.path.join(models_path, \"word2idx.pkl\")\n",
    "\n",
    "# Load word2idx from word2idx.pkl\n",
    "try:\n",
    "    with open(word2idx_path, 'rb') as f:\n",
    "        word2idx = pickle.load(f)\n",
    "    print(\"word2idx loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading word2idx: {e}. Make sure the file exists at {word2idx_path}.\")\n",
    "    raise\n",
    "\n",
    "# Load embedding matrix\n",
    "try:\n",
    "    embedding_matrix = torch.load(embedding_matrix_path, map_location=torch.device('cpu'), weights_only=True)\n",
    "    print(\"Embedding matrix loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading embedding matrix: {e}. Make sure the file exists at {embedding_matrix_path}.\")\n",
    "    raise\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = len(word2idx)\n",
    "output_size = len(word2idx)\n",
    "hidden_size = 256  # Adjusted according to training\n",
    "\n",
    "# Initialize the models\n",
    "encoder = Encoder(input_size, embedding_matrix, hidden_size).cpu()\n",
    "decoder = DecoderWithAttention(output_size, embedding_matrix, hidden_size).cpu()\n",
    "model = Seq2SeqWithAttention(encoder, decoder, device='cpu').cpu()\n",
    "\n",
    "# Load pre-trained weights\n",
    "try:\n",
    "    encoder.load_state_dict(torch.load(encoder_weights_path, map_location=torch.device('cpu'), weights_only=True))\n",
    "    decoder.load_state_dict(torch.load(decoder_weights_path, map_location=torch.device('cpu'), weights_only=True))\n",
    "    model.load_state_dict(torch.load(seq2seq_weights_path, map_location=torch.device('cpu'), weights_only=True))\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading model weights: {e}. Make sure the model files exist at {models_path}.\")\n",
    "    raise\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "model.eval()\n",
    "\n",
    "print(\"Models loaded successfully.\")\n"
   ],
   "id": "37b8d36db4d36908",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory set to: C:\\Users\\Girija\\Documents\\GitHub\\NLP-Driven-Rasa-Assiste-ChatBot360\n",
      "Looking for 'utils' folder at: C:\\Users\\Girija\\Documents\\GitHub\\NLP-Driven-Rasa-Assiste-ChatBot360\\utils\n",
      "Looking for 'encoder.py' at: C:\\Users\\Girija\\Documents\\GitHub\\NLP-Driven-Rasa-Assiste-ChatBot360\\utils\\encoder.py\n",
      "Encoder loaded successfully from C:\\Users\\Girija\\Documents\\GitHub\\NLP-Driven-Rasa-Assiste-ChatBot360\\utils\\encoder.py\n",
      "Decoder loaded successfully from C:\\Users\\Girija\\Documents\\GitHub\\NLP-Driven-Rasa-Assiste-ChatBot360\\utils\\decoder.py\n",
      "Seq2Seq model loaded successfully from C:\\Users\\Girija\\Documents\\GitHub\\NLP-Driven-Rasa-Assiste-ChatBot360\\utils\\seq2seq.py\n",
      "word2idx loaded successfully.\n",
      "Embedding matrix loaded successfully.\n",
      "Model weights loaded successfully.\n",
      "Models loaded successfully.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T20:07:55.013819Z",
     "start_time": "2024-11-30T20:07:54.998925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# %% Section 3: Define Tokenization and Preprocessing Functions\n",
    "\n",
    "def preprocess_input(text, word2idx, max_len=20):\n",
    "    \"\"\"\n",
    "    Preprocess input text for the Seq2Seq model.\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        word2idx (dict): Word to index mapping.\n",
    "        max_len (int): Maximum length of the token sequence.\n",
    "    Returns:\n",
    "        torch.Tensor: The tokenized and padded input sequence.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    sequence = [word2idx.get(token, word2idx['<UNK>']) for token in tokens]\n",
    "    sequence = [word2idx['<SOS>']] + sequence + [word2idx['<EOS>']]\n",
    "    sequence = sequence[:max_len] + [word2idx['<PAD>']] * (max_len - len(sequence))\n",
    "    return torch.tensor(sequence, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n"
   ],
   "id": "86d286c5cb6833e5",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T20:09:48.198280Z",
     "start_time": "2024-11-30T20:09:48.175445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_response(input_text, model, word2idx, max_len=20):\n",
    "    \"\"\"\n",
    "    Generate a response from the trained Seq2Seq model.\n",
    "    Args:\n",
    "        input_text (str): The input query.\n",
    "        model (Seq2SeqWithAttention): Trained Seq2Seq model.\n",
    "        word2idx (dict): Word to index mapping.\n",
    "        max_len (int): Maximum length of the response sequence.\n",
    "    Returns:\n",
    "        str: Generated response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Tokenize and preprocess the input\n",
    "        input_tensor = preprocess_input(input_text, word2idx, max_len=max_len)\n",
    "        input_tensor = input_tensor.to('cpu')\n",
    "\n",
    "        print(f\"Preprocessed input tensor: {input_tensor}\")\n",
    "\n",
    "        # Prepare a placeholder target tensor (start with <SOS> token)\n",
    "        target_tensor = torch.tensor([word2idx['<SOS>']], dtype=torch.long).unsqueeze(0).to('cpu')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass through the Seq2Seq model\n",
    "            output = model(input_tensor, target_tensor, teacher_forcing_ratio=0.0)\n",
    "\n",
    "        # Debugging: print output shape and values\n",
    "        print(f\"Model output tensor shape: {output.shape}\")\n",
    "        print(f\"Model output tensor: {output}\")\n",
    "\n",
    "        # Extract the predicted tokens\n",
    "        predicted_indices = output.argmax(2).squeeze().tolist()\n",
    "\n",
    "        # Ensure `predicted_indices` is a list\n",
    "        if isinstance(predicted_indices, int):\n",
    "            predicted_indices = [predicted_indices]\n",
    "\n",
    "        # Correcting the response generation logic:\n",
    "        response_tokens = []\n",
    "        for idx in predicted_indices:\n",
    "            # Retrieve the token corresponding to the index\n",
    "            token = next((key for key, value in word2idx.items() if value == idx), None)\n",
    "            # Append the token to the response if it's not a special token\n",
    "            if token and token not in ['<SOS>', '<EOS>', '<PAD>']:\n",
    "                response_tokens.append(token)\n",
    "\n",
    "        # Debugging: print response tokens\n",
    "        print(f\"Response tokens: {response_tokens}\")\n",
    "\n",
    "        # Reconstruct response text\n",
    "        response = ' '.join(response_tokens)\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during response generation: {e}.\")\n",
    "        return \"\"\n"
   ],
   "id": "efa82fd0bc4f743f",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T20:09:48.976594Z",
     "start_time": "2024-11-30T20:09:48.948959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# %% Section 5: Testing the Model with a Predefined Input\n",
    "\n",
    "# Example predefined input\n",
    "predefined_input = \"Hello, how can I assist you today?\"\n",
    "\n",
    "# Generate response\n",
    "try:\n",
    "    response = generate_response(predefined_input, model, word2idx)\n",
    "    print(f\"Input: {predefined_input}\")\n",
    "    print(f\"Response: {response}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during response generation: {e}.\")"
   ],
   "id": "c5d7b5945f1588f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed input tensor: tensor([[400001,  13075,      1,    197,     86,     41,   4281,     81,    373,\n",
      "            188, 400002, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
      "         400000, 400000]])\n",
      "Model output tensor shape: torch.Size([1, 1, 400004])\n",
      "Model output tensor: tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Response tokens: ['the']\n",
      "Input: Hello, how can I assist you today?\n",
      "Response: the\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T20:11:33.097095Z",
     "start_time": "2024-11-30T20:11:33.037313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% Section 5: Testing the Model with a Predefined Set of Inputs\n",
    "\n",
    "# Predefined set of inputs to test the model\n",
    "predefined_inputs = [\n",
    "    \"Hello, how can I assist you today?\",\n",
    "    \"What is your name?\",\n",
    "    \"Can you tell me a joke?\",\n",
    "    \"How is the weather today?\",\n",
    "    \"What time is it now?\"\n",
    "]\n",
    "\n",
    "# Generate responses for each predefined input\n",
    "for predefined_input in predefined_inputs:\n",
    "    try:\n",
    "        response = generate_response(predefined_input, model, word2idx)\n",
    "        print(f\"Input: {predefined_input}\")\n",
    "        print(f\"Response: {response}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during response generation for input '{predefined_input}': {e}.\")\n"
   ],
   "id": "9fced7f6fff7f6d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed input tensor: tensor([[400001,  13075,      1,    197,     86,     41,   4281,     81,    373,\n",
      "            188, 400002, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
      "         400000, 400000]])\n",
      "Model output tensor shape: torch.Size([1, 1, 400004])\n",
      "Model output tensor: tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Response tokens: ['the']\n",
      "Input: Hello, how can I assist you today?\n",
      "Response: the\n",
      "\n",
      "Preprocessed input tensor: tensor([[400001,    102,     14,    392,    311,    188, 400002, 400000, 400000,\n",
      "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
      "         400000, 400000]])\n",
      "Model output tensor shape: torch.Size([1, 1, 400004])\n",
      "Model output tensor: tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Response tokens: ['the']\n",
      "Input: What is your name?\n",
      "Response: the\n",
      "\n",
      "Preprocessed input tensor: tensor([[400001,     86,     81,   1361,    285,      7,   7351,    188, 400002,\n",
      "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
      "         400000, 400000]])\n",
      "Model output tensor shape: torch.Size([1, 1, 400004])\n",
      "Model output tensor: tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Response tokens: ['the']\n",
      "Input: Can you tell me a joke?\n",
      "Response: the\n",
      "\n",
      "Preprocessed input tensor: tensor([[400001,    197,     14,      0,   1620,    373,    188, 400002, 400000,\n",
      "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
      "         400000, 400000]])\n",
      "Model output tensor shape: torch.Size([1, 1, 400004])\n",
      "Model output tensor: tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Response tokens: ['the']\n",
      "Input: How is the weather today?\n",
      "Response: the\n",
      "\n",
      "Preprocessed input tensor: tensor([[400001,    102,     79,     14,     20,    114,    188, 400002, 400000,\n",
      "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
      "         400000, 400000]])\n",
      "Model output tensor shape: torch.Size([1, 1, 400004])\n",
      "Model output tensor: tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Response tokens: ['the']\n",
      "Input: What time is it now?\n",
      "Response: the\n",
      "\n"
     ]
    }
   ],
   "execution_count": 43
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
