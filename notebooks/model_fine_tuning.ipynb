{
 "cells": [
  {
   "metadata": {
    "id": "2e75f2da2ce2abd0"
   },
   "cell_type": "markdown",
   "source": [
    "#### **Section 1: Import Libraries**\n",
    "\n",
    "We import all the necessary libraries. Notice that we no longer need to import `gensim`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "initial_id",
    "outputId": "d3a060b8-619a-40c6-de26-4c390c6996b0",
    "ExecuteTime": {
     "end_time": "2024-11-30T17:20:47.244010Z",
     "start_time": "2024-11-30T17:20:30.055346Z"
    }
   },
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Import word_tokenize explicitly from NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Observations:\n",
    "# - Added an import for `word_tokenize` explicitly after downloading the NLTK 'punkt' resource.\n",
    "# - The word_tokenize function is now available globally in the script, and the error will no longer occur.\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Girija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "id": "6dcc650cdf0e0c18"
   },
   "cell_type": "markdown",
   "source": [
    "#### **Section 2: Load and Prepare Data**\n",
    "\n",
    "The data loading process remains the same.\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "75d03ab8849a9fcd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "outputId": "9a7e72ef-d851-4100-af36-8b23f0042b16",
    "ExecuteTime": {
     "end_time": "2024-11-30T18:33:48.130909Z",
     "start_time": "2024-11-30T18:33:47.690638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Load the Processed Data\n",
    "#file_path = \"../data/processed/customer_support_dataset_processed.csv\"  # for complete set\n",
    "file_path = \"../data/processed/customer_support_train_dataset_processed_10%.csv\" # for 10% for the complete set - for simple training\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the dataset contains essential columns\n",
    "if 'customer_query_cleaned' not in df.columns or 'support_response_cleaned' not in df.columns:\n",
    "    raise ValueError(\"Dataset missing required columns: 'customer_query_cleaned' and 'support_response_cleaned'\")\n",
    "\n",
    "# Split data into input and output pairs\n",
    "queries = df['customer_query_cleaned']\n",
    "responses = df['support_response_cleaned']\n",
    "\n",
    "# Split dataset into training and validation sets (90% train, 10% validation)\n",
    "train_queries, val_queries, train_responses, val_responses = train_test_split(\n",
    "    queries, responses, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Observations:\n",
    "# - Loaded and validated the cleaned dataset.\n",
    "# - Split the data into training and validation sets, which is critical for model evaluation and avoiding overfitting.\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Step 2: Load the Processed Data\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#file_path = \"../data/processed/customer_support_dataset_processed.csv\"  # for complete set\u001B[39;00m\n\u001B[0;32m      3\u001B[0m file_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../data/processed/customer_support_train_dataset_processed_10\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;66;03m# for 10% for the complete set - for simple training\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mread_csv(file_path)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Ensure the dataset contains essential columns\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcustomer_query_cleaned\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msupport_response_cleaned\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns:\n",
      "\u001B[1;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "id": "c037ca552bc214fb"
   },
   "cell_type": "markdown",
   "source": [
    "#### **Section 3: Load Pre-trained GloVe Embeddings Without Gensim**\n",
    "\n",
    "Instead of using Gensim, you will manually download the GloVe embeddings, read them, and then use them to create the embedding matrix.\n",
    "\n",
    "##### **3.1 Download GloVe Embeddings Manually**\n",
    "\n",
    "- You can download GloVe embeddings manually from the [GloVe Website](https://nlp.stanford.edu/projects/glove/). Choose, for example, the **glove.6B.zip** file and extract it.\n",
    "- It contains multiple files like `glove.6B.50d.txt`, `glove.6B.100d.txt`, etc. We'll use `glove.6B.100d.txt` for 100-dimensional word embeddings.\n",
    "\n",
    "##### **3.2 Load GloVe Embeddings in Python**\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "3f3004d6c087e04c",
    "ExecuteTime": {
     "end_time": "2024-11-30T17:21:42.502767Z",
     "start_time": "2024-11-30T17:20:49.718608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3: Load Pre-trained GloVe Embeddings (Without Gensim)\n",
    "embedding_dim = 100\n",
    "glove_path = \"../glove.6B.100d.txt\"  # Path to the downloaded GloVe file\n",
    "\n",
    "# Initialize word2idx and embedding matrix lists\n",
    "word2idx = {}\n",
    "embedding_matrix = []\n",
    "\n",
    "# Open the GloVe file and read the embeddings\n",
    "print(\"Loading pre-trained GloVe embeddings (may take a few minutes)...\")\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        word2idx[word] = idx\n",
    "        embedding_matrix.append(vector)\n",
    "\n",
    "# Add special tokens with random embeddings\n",
    "special_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "for token in special_tokens:\n",
    "    word2idx[token] = len(embedding_matrix)\n",
    "    embedding_matrix.append(np.random.normal(size=(embedding_dim,)))\n",
    "\n",
    "# Convert embedding matrix to a tensor\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Observations:\n",
    "# - Loaded GloVe embeddings manually using Python without Gensim.\n",
    "# - Created an embedding matrix and added random embeddings for special tokens.\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained GloVe embeddings (may take a few minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Girija\\AppData\\Local\\Temp\\ipykernel_12848\\1172931956.py:26: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "id": "f662c4e8081601c5"
   },
   "cell_type": "markdown",
   "source": [
    "#### **Section 4: Custom Dataset and DataLoader**\n",
    "\n",
    "This section remains the same. The custom dataset is responsible for tokenizing the input and padding it to a fixed length.\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "3091c51ad71de2dc",
    "ExecuteTime": {
     "end_time": "2024-11-30T17:21:43.397712Z",
     "start_time": "2024-11-30T17:21:43.246275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 4: Custom Dataset and DataLoader\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, queries, responses, word2idx, max_len=20):\n",
    "        # Reset the index of queries and responses to ensure valid indexing\n",
    "        self.queries = queries.reset_index(drop=True).fillna(\"\")  # Replace NaN with empty string\n",
    "        self.responses = responses.reset_index(drop=True).fillna(\"\")  # Replace NaN with empty string\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert text to token ids and pad/truncate to max_len\n",
    "        try:\n",
    "            query = self._text_to_sequence(self.queries[idx])\n",
    "            response = self._text_to_sequence(self.responses[idx])\n",
    "        except KeyError:\n",
    "            print(f\"KeyError: Index {idx} out of bounds for dataset length {len(self.queries)}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error at index {idx}: {e}\")\n",
    "            raise\n",
    "        return torch.tensor(query, dtype=torch.long), torch.tensor(response, dtype=torch.long)\n",
    "\n",
    "    def _text_to_sequence(self, text):\n",
    "        # Handle non-string inputs\n",
    "        if not isinstance(text, str):\n",
    "            print(f\"Invalid input detected: {text} (type: {type(text)}). Converting to empty string.\")\n",
    "            text = \"\"\n",
    "\n",
    "        tokens = word_tokenize(text)  # Tokenize the text\n",
    "        sequence = [self.word2idx.get(token, self.word2idx['<UNK>']) for token in tokens]\n",
    "        sequence = [self.word2idx['<SOS>']] + sequence + [self.word2idx['<EOS>']]\n",
    "        sequence = sequence[:self.max_len] + [self.word2idx['<PAD>']] * (self.max_len - len(sequence))\n",
    "        return sequence\n",
    "\n",
    "# DataLoader instances for training and validation\n",
    "train_dataset = ChatDataset(train_queries, train_responses, word2idx)\n",
    "val_dataset = ChatDataset(val_queries, val_responses, word2idx)\n",
    "\n",
    "# DataLoader setup with reduced batch size to lower memory consumption\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0, pin_memory=True)\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "id": "eab1e86d7c52ad56"
   },
   "cell_type": "markdown",
   "source": [
    "#### **Section 5: Encoder-Decoder Model Design with Attention**\n",
    "\n",
    "The encoder and decoder design with attention remains largely unchanged, except that we use the manually loaded GloVe embeddings.\n",
    "\n",
    "##### **5.1 Encoder Definition**\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "939c3afd72e34fc9",
    "ExecuteTime": {
     "end_time": "2024-11-30T17:21:44.153070Z",
     "start_time": "2024-11-30T17:21:44.127478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encoder Definition\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_matrix, hidden_size, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.rnn = nn.LSTM(embedding_matrix.size(1), hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        embedded = self.embedding(x)  # embedded: [batch_size, seq_len, embedding_dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# Observations:\n",
    "# - The encoder uses pre-trained GloVe embeddings loaded manually.\n",
    "# - The embeddings are not frozen (`freeze=False`), meaning they will be fine-tuned during training.\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "id": "67674d72ef93f897"
   },
   "cell_type": "markdown",
   "source": [
    "##### **5.2 Decoder with Attention Definition**\n",
    "\n",
    "The decoder is modified to include the attention layer for better context representation.\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "bc5add52a0f8320c",
    "ExecuteTime": {
     "end_time": "2024-11-30T17:21:44.830340Z",
     "start_time": "2024-11-30T17:21:44.789339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Attention Mechanism (Make sure it's defined before the decoder)\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Linear layers to compute alignment scores and convert to attention weights\n",
    "        self.attention = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [batch_size, hidden_size]\n",
    "        # encoder_outputs: [batch_size, seq_len, hidden_size]\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        seq_len = encoder_outputs.shape[1]\n",
    "\n",
    "        # Repeat hidden state seq_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "\n",
    "        # Concatenate encoder outputs with the repeated hidden state\n",
    "        energy = torch.tanh(self.attention(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # Calculate attention scores\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "\n",
    "        # Apply softmax to calculate attention weights\n",
    "        return torch.softmax(attention, dim=1)\n",
    "\n",
    "# Decoder with Attention Definition (After Attention class is defined)\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, output_size, embedding_matrix, hidden_size, num_layers=1):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size + embedding_matrix.size(1), hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell, encoder_outputs):\n",
    "        # x: [batch_size], hidden, cell: [num_layers, batch_size, hidden_size], encoder_outputs: [batch_size, seq_len, hidden_size]\n",
    "        x = x.unsqueeze(1)  # Add time dimension: [batch_size, 1]\n",
    "        embedded = self.embedding(x)  # embedded: [batch_size, 1, embedding_dim]\n",
    "\n",
    "        # Calculate attention weights and apply to encoder outputs to get context vector\n",
    "        attention_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "        attention_weights = attention_weights.unsqueeze(1)  # [batch_size, 1, seq_len]\n",
    "        context = torch.bmm(attention_weights, encoder_outputs)  # [batch_size, 1, hidden_size]\n",
    "\n",
    "        # Concatenate the context vector with the embedded input word\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)  # [batch_size, 1, hidden_size + embedding_dim]\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "\n",
    "        # Use the output of RNN and context vector for prediction\n",
    "        prediction = self.fc(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))  # [batch_size, output_size]\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "# Observations:\n",
    "# - The `Attention` class must be defined before it is used by `DecoderWithAttention`.\n",
    "# - This ensures there is no `NameError` when defining the decoder.\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "id": "bc3c0e0c9369114b"
   },
   "cell_type": "markdown",
   "source": [
    "#### **Section 6: Seq2Seq Model Class with Attention Decoder**\n",
    "\n",
    "The Seq2Seq class integrates the **Encoder** and **DecoderWithAttention** to generate responses.\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "5bdab3eec446ee45",
    "ExecuteTime": {
     "end_time": "2024-11-30T17:21:45.517984Z",
     "start_time": "2024-11-30T17:21:45.492914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Seq2Seq Model Class with Attention Decoder\n",
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2SeqWithAttention, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size = source.shape[0]\n",
    "        target_len = target.shape[1]\n",
    "        output_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, target_len, output_size).to(self.device)\n",
    "\n",
    "        # Pass input through the encoder\n",
    "        encoder_outputs, hidden, cell = self.encoder(source)\n",
    "\n",
    "        # First input to the decoder is the <SOS> token\n",
    "        input = target[:, 0]  # <SOS> token for each batch\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Pass the input, hidden state, and encoder outputs to the decoder\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            # Determine the next input using teacher forcing\n",
    "            top1 = output.argmax(1)\n",
    "            input = target[:, t] if np.random.random() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Observations:\n",
    "# - The Seq2Seq model class integrates the encoder and decoder and passes encoder outputs to the decoder for attention.\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "id": "7d9e2c5fd238a550"
   },
   "cell_type": "markdown",
   "source": [
    "#### **Section 7: Training the Model with Attention**\n",
    "\n",
    "This initial step is designed to quickly train the model with only 1 epoch, allowing you to verify that everything is working properly before committing to a longer training time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download('punkt_tab')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhpYR2d-JsfX",
    "outputId": "9e7d85de-81f9-4e97-bcb9-c405740a3011"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ]
  },
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-30T12:26:26.744594Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b2a1ff4e2188428",
    "outputId": "9e236fd8-cb5d-47cc-b586-250e7093fb50"
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = len(word2idx)  # Vocabulary size for input\n",
    "output_size = len(word2idx)  # Vocabulary size for output\n",
    "hidden_size = 256  # Reduced hidden size\n",
    "embedding_dim = 100  # Reduced embedding dimension to lower memory usage\n",
    "num_epochs = 1  # Training for one epoch initially\n",
    "learning_rate = 0.001  # Learning rate for the optimizer\n",
    "\n",
    "\n",
    "# Instantiate embedding matrix on CPU to save GPU memory\n",
    "embedding_matrix = torch.randn(len(word2idx), embedding_dim)  # Initialize embeddings with reduced dimensions\n",
    "\n",
    "# Instantiate encoder, decoder with attention, and Seq2Seq model on CPU\n",
    "encoder = Encoder(input_size, embedding_matrix, hidden_size).cpu()\n",
    "decoder = DecoderWithAttention(output_size, embedding_matrix, hidden_size).cpu()\n",
    "model = Seq2SeqWithAttention(encoder, decoder, device).cpu()\n",
    "\n",
    "# Move encoder, decoder, and model to GPU (if available)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "scaler = GradScaler(device='cuda')  # Mixed precision gradient scaler\n",
    "\n",
    "# Gradient accumulation steps\n",
    "accumulation_steps = 2  # Accumulate gradients over 2 mini-batches\n",
    "\n",
    "# Training loop with validation and checkpointing\n",
    "best_val_loss = float('inf')  # Initialize the best validation loss\n",
    "\n",
    "# Initial training for 1 epoch\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = 0  # Initialize epoch loss\n",
    "\n",
    "    # Training Phase with Progress Bar\n",
    "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        for i, (source, target) in enumerate(train_loader):\n",
    "            source = source.to(device)  # Move source to GPU/CPU\n",
    "            target = target.to(device)  # Move target to GPU/CPU\n",
    "\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "\n",
    "            # Mixed Precision Training\n",
    "            with autocast(device_type='cuda'):\n",
    "                output = model(source, target)  # Forward pass\n",
    "\n",
    "                # Reshape output and target for loss calculation\n",
    "                output = output[:, 1:].reshape(-1, output.shape[-1])  # Exclude <SOS> token\n",
    "                target = target[:, 1:].reshape(-1)  # Exclude <SOS> token\n",
    "\n",
    "                # Loss scaling for accumulation\n",
    "                loss = criterion(output, target) / accumulation_steps\n",
    "\n",
    "            # Scale and backpropagate\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Step optimizer after accumulation steps\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)  # Update model weights\n",
    "                scaler.update()  # Update scaler for next step\n",
    "\n",
    "            epoch_loss += loss.item() * accumulation_steps  # Track cumulative loss\n",
    "            pbar.set_postfix({\"Batch Loss\": loss.item() * accumulation_steps})  # Update progress bar\n",
    "            pbar.update(1)  # Increment progress bar\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)  # Calculate average training loss\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for source, target in val_loader:\n",
    "            source = source.to(device)  # Move source to GPU/CPU\n",
    "            target = target.to(device)  # Move target to GPU/CPU\n",
    "\n",
    "            output = model(source, target)  # Forward pass\n",
    "            output = output[:, 1:].reshape(-1, output.shape[-1])  # Reshape output\n",
    "            target = target[:, 1:].reshape(-1)  # Reshape target\n",
    "\n",
    "            loss = criterion(output, target)  # Calculate validation loss\n",
    "            val_loss += loss.item()  # Accumulate validation loss\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)  # Calculate average validation loss\n",
    "\n",
    "    # Save the best model based on validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"seq2seq_attention_best_model.pth\")  # Save model\n",
    "        print(f\"Model improved. Saving the best model with validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Observations:\n",
    "# - Used mixed precision training using `autocast()` and `GradScaler()` to improve training speed and reduce memory usage.\n",
    "# - Reduced `hidden_size` and `embedding_dim` to reduce model memory footprint.\n",
    "# - Initialized encoder and decoder on CPU before transferring to GPU.\n",
    "# - Reduced batch size for training and validation to further mitigate memory issues.\n",
    "# - Used `tqdm` to monitor progress during training and validation.\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1/1: 100%|██████████| 7166/7166 [1:46:45<00:00,  1.12batch/s, Batch Loss=5.07]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model improved. Saving the best model with validation loss: 4.9798\n",
      "Epoch [1/1], Train Loss: 5.3375, Val Loss: 4.9798\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "id": "6251c82307d432ec"
   },
   "cell_type": "markdown",
   "source": [
    "#### Step 8: Load and Continue Training the Pre-Trained Model (Fine-Tuning)\n",
    "This step is about fine-tuning the model that was trained for a single epoch in Step 7. The fine-tuning process can be carried out by loading the pre-trained model and training it further for more epochs with adjusted hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# from previous code\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = len(word2idx)  # Vocabulary size for input\n",
    "output_size = len(word2idx)  # Vocabulary size for output\n",
    "hidden_size = 256  # Reduced hidden size\n",
    "embedding_dim = 100  # Reduced embedding dimension to lower memory usage\n",
    "num_epochs = 1  # Training for one epoch initially\n",
    "learning_rate = 0.001  # Learning rate for the optimizer\n"
   ],
   "metadata": {
    "id": "PfVUAfmkpz04",
    "ExecuteTime": {
     "end_time": "2024-11-30T17:22:27.992188Z",
     "start_time": "2024-11-30T17:22:27.981192Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "4ad820925a16868f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1c455345-077d-4d64-8fa3-07c7bda22c33",
    "ExecuteTime": {
     "end_time": "2024-11-30T17:27:54.939540Z",
     "start_time": "2024-11-30T17:25:57.047214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "#### Step 8: Load and Continue Training the Pre-Trained Model (Fine-Tuning)\n",
    "\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters for further fine-tuning\n",
    "num_additional_epochs = 2  # Number of additional epochs for fine-tuning\n",
    "fine_tune_learning_rate = learning_rate / 10  # Reduce the learning rate for more stable training\n",
    "batch_size = 8  # Reduced batch size to avoid memory issues\n",
    "accumulation_steps = 16  # Reduced number of gradient accumulation steps\n",
    "\n",
    "\n",
    "# Path for saving to Google Drive\n",
    "tuned_model_path = 'seq2seq_attention_best_model_finetuned.pth'\n",
    "\n",
    "# Instantiate embedding matrix on CPU to save GPU memory\n",
    "embedding_dim = 100  # Reduced embedding dimension (use the same as before)\n",
    "embedding_matrix = torch.randn(len(word2idx), embedding_dim)  # Create embedding matrix\n",
    "\n",
    "# Instantiate encoder, decoder with attention, and Seq2Seq model\n",
    "encoder = Encoder(input_size, embedding_matrix, hidden_size).cpu()\n",
    "decoder = DecoderWithAttention(output_size, embedding_matrix, hidden_size).cpu()\n",
    "model = Seq2SeqWithAttention(encoder, decoder, device).cpu()\n",
    "\n",
    "# Move models to GPU\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the pre-trained model checkpoint\n",
    "checkpoint_path = \"../seq2seq_attention_best_model.pth\"\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "\n",
    "# Put the model in training mode to resume training\n",
    "model.train()\n",
    "\n",
    "# Define optimizer and loss function for further fine-tuning\n",
    "optimizer = optim.Adam(model.parameters(), lr=fine_tune_learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "scaler = GradScaler(device='cuda')  # Mixed precision gradient scaler\n",
    "\n",
    "# DataLoader instances for training and validation (reuse the setup)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Set PyTorch CUDA allocation configuration for flexible memory management\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Continue training the model for additional epochs\n",
    "best_val_loss = float('inf')  # Keep track of the best validation loss\n",
    "\n",
    "for epoch in range(num_additional_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Training Phase with Progress Bar\n",
    "    with tqdm(total=len(train_loader), desc=f\"Fine-Tuning Epoch {epoch + 1}/{num_additional_epochs}\", unit=\"batch\") as pbar:\n",
    "        for i, (source, target) in enumerate(train_loader):\n",
    "            source = source.to(device)  # Move source to GPU/CPU\n",
    "            target = target.to(device)  # Move target to GPU/CPU\n",
    "\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "\n",
    "            # Mixed Precision Training\n",
    "            with autocast(device_type='cuda'):\n",
    "                output = model(source, target)  # Forward pass\n",
    "\n",
    "                # Reshape output and target for loss calculation\n",
    "                output = output[:, 1:].reshape(-1, output.shape[-1])  # Exclude <SOS> token\n",
    "                target = target[:, 1:].reshape(-1)  # Exclude <SOS> token\n",
    "\n",
    "                # Loss scaling for accumulation\n",
    "                loss = criterion(output, target) / accumulation_steps\n",
    "\n",
    "            # Scale and backpropagate\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Step optimizer after accumulation steps\n",
    "            if (i + 1) % accumulation_steps == 0 or i == len(train_loader) - 1:\n",
    "                scaler.step(optimizer)  # Update model weights\n",
    "                scaler.update()  # Update scaler for next step\n",
    "\n",
    "            epoch_loss += loss.item() * accumulation_steps\n",
    "            pbar.set_postfix({\"Batch Loss\": loss.item() * accumulation_steps})  # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Manually empty CUDA cache to avoid memory buildup\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for source, target in val_loader:\n",
    "            source = source.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            output = model(source, target)\n",
    "            output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "            target = target[:, 1:].reshape(-1)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Save the best model based on validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), tuned_model_path)\n",
    "        print(f\"Model improved. Saving new best model with validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    print(f'Additional Epoch [{epoch + 1}/{num_additional_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Observations:\n",
    "# - **Batch Size Reduced**: Lowered batch size to `8` to reduce memory consumption.\n",
    "# - **Gradient Accumulation Steps**: Reduced accumulation steps to `16` to balance memory consumption.\n",
    "# - **Mixed Precision Training**: Continued use of mixed precision with `autocast()` for efficient memory management.\n",
    "# - **CUDA Cache Management**: Added `torch.cuda.empty_cache()` to manually clear the CUDA memory to prevent accumulation of unnecessary memory usage.\n",
    "# - **Flexible Memory Allocation**: Used `PYTORCH_CUDA_ALLOC_CONF` for expandable memory segments to reduce fragmentation issues.\n",
    "# - **Model Saving in Google Drive**: Saved the model directly to Google Drive for convenience.\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Girija\\AppData\\Local\\Temp\\ipykernel_12848\\3070623499.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
      "C:\\Users\\Girija\\Documents\\GitHub\\NLP-Driven-Rasa-Assiste-ChatBot360\\.venv\\lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "Fine-Tuning Epoch 1/2:   0%|          | 0/14332 [00:00<?, ?batch/s]C:\\Users\\Girija\\Documents\\GitHub\\NLP-Driven-Rasa-Assiste-ChatBot360\\.venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Fine-Tuning Epoch 1/2:   0%|          | 6/14332 [01:48<72:06:24, 18.12s/batch, Batch Loss=4.95]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 85\u001B[0m\n\u001B[0;32m     82\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(output, target) \u001B[38;5;241m/\u001B[39m accumulation_steps\n\u001B[0;32m     84\u001B[0m \u001B[38;5;66;03m# Scale and backpropagate\u001B[39;00m\n\u001B[1;32m---> 85\u001B[0m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;66;03m# Step optimizer after accumulation steps\u001B[39;00m\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m accumulation_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_loader) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\NLP-Driven-Rasa-Assiste-ChatBot360\\.venv\\lib\\site-packages\\torch\\_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    580\u001B[0m     )\n\u001B[1;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\NLP-Driven-Rasa-Assiste-ChatBot360\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\NLP-Driven-Rasa-Assiste-ChatBot360\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    826\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    827\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "id": "14186366a39e7577"
   },
   "cell_type": "markdown",
   "source": []
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "55f70ee4ed323068"
   },
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "f5f40c097e37b3f3"
   },
   "cell_type": "markdown",
   "source": []
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "f8ca0b8d579ba7e5"
   },
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "9bf0fa726c9116b4"
   },
   "cell_type": "markdown",
   "source": []
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "5cc11740f49df840"
   },
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
